{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ###  Assignment : Bivariate Gaussian + Linear Transformation of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Explain in your own words what effect does the choice of Covariance matrix have on the Bivariate Gaussian (compare spherical, elliptical). What does it mean when the covariance matrix is not diagonal? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1: circular\n",
    "When we assume the covariance between two variables is zero(independent variable assumption) and the varainces of these idependent variable is equal then the contour plot takes the circular form.\n",
    "\n",
    "case 2 : elliptical, aligned with the main axes(x,y)\n",
    "When these two variables are independent and the variances are different then the variable with more variance value take the major axis of ellipse.\n",
    "\n",
    "case 3: elliptical, inclined with certain angle with main axes(x,y)\n",
    "When the two variables are correlated the angle of inclination of the ellipse is given by the eigen vector of the covariance matrix of these two variable and the width of major and minor axis is given by the corresponding eigen values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Consider a random vector X and linear transformation f(X)=aX+b=Y\n",
    "where X and b vector shows normal distribution with zero mean.\n",
    "1. Find the distribution of f(x), and the parameters\n",
    "2. Find the distribution of z=transpose[x,y], and the parameters\n",
    "\n",
    "### Task 1\n",
    "##### The parameter of y\n",
    "\\begin{align}\n",
    "\\mu_y = E[y] = E(Ax+b) \\\\\n",
    "= A E[x]+E[b]\n",
    "= A \\mu_x \\\\\n",
    "\\mu_y = A \\mu_x\n",
    "\\end{align}\n",
    "##### Now  the covariance\n",
    "\\begin{align}\n",
    "\\sum_{y} = Var(Ax+b) = Var(Ax)+Var(b) = Var(Ax) \\\\\n",
    "= E[(A(x-\\mu)((x-\\mu)^t))]\n",
    "= E(A(x_\\mu)(x-\\mu)^tA^t) = AE[(x-\\mu)(x-u)^t]A^t \\\\\n",
    "= A(Var(x)A^t) \\\\\n",
    "= A \\sum_xA^t\n",
    "\\end{align}\n",
    "\n",
    "### Task 2\n",
    "##### Joint distribution\n",
    "$$\\vec{Z} = [\\vec{x},\\vec{y}]^t$$\n",
    "\\begin{align}\n",
    "\\mu_z = \\begin{pmatrix} \\mu_x \\\\ \\mu_y \\end{pmatrix} \\\\\n",
    "=  \\begin{pmatrix} \\mu_x \\\\ A\\mu_x \\end{pmatrix}\n",
    "\\end{align}\n",
    "Now the covariance\n",
    "\\begin{align}\n",
    "Cov(\\vec{X},\\vec{Y}) = E[\\vec{X} \\vec{Y}^T]-E[X].E[Y] \\\\\n",
    "= E[X(AX+b)^t]-E[X].E[(AX+b)^t]\\\\\n",
    "= E[X(X^TA^T+b^t)]-\\mu_x. \\mu_x^t . A^t \\\\\n",
    "= E[XX^tA^t + Xb^t] - \\mu_x. \\mu_x^t . A^t \\\\\n",
    "= E[XX^tA^T] -\\mu_x. \\mu_x^t . A^t \\\\\n",
    "= A^TE[XX^t]-\\mu_x. \\mu_x^t . A^t \\\\\n",
    "= (E[XX^t]-\\mu_x. \\mu_x^t)A^t\n",
    "= (E[XX^t]-E[X]E[X^t]) A^t\\\\\n",
    "= \\sum_xA^t\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 What is the meaning of Mahalanobis distance? What is the relation of this to the eigenvalues of the Covariance matrix? Draw a sketch either in Python or by hand for the Bivariate case (K=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mahalanobis distance is the measure of the distance between a point and multivariate distribution. This distance is zero if the point is at the mean of distribution, and grows as the point moves away from the mean along each principal component axis. Alternatively,  can also be defined as a dissimilarity measure between two random vectors of the same distribution.'\n",
    "\n",
    "We need eigen values and eigen vectors to project the distribution to their pricipal components eliminates correalation and further applying whitening transformation gives unit variance for both variable and allows Mahalaobis distance calculation equivalent to the euclidean distance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
